[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biometry",
    "section": "",
    "text": "Aloha mai!\nThis is the course website for ZOOL XYZ Biometry at the University of Hawaiʻi Mānoa.",
    "crumbs": [
      "Aloha mai!"
    ]
  },
  {
    "objectID": "likelihood.html",
    "href": "likelihood.html",
    "title": "1  Likelihood, AIC, and likelihood ratio tests",
    "section": "",
    "text": "1.1 Let’s start with a model\nLet’s start with a Poisson GLM and a made-up dataset to go along with it. Our data will just be y (the response variable) and x (the explanatory variable). We will assume y is the count of something (e.g. species abundance) and x is some kind of continuous numerical variable like temperature or rainfall, etc.\nLet’s simulate some data\n# uniformly distributed explanatory variable\nx &lt;- runif(100)\n\n# calculate the mean of `y` as a function of `x`\n# (don't worry about interpreting the right hand side right now)\nlambda &lt;- exp(1 + 2 * x)\n\n# finally calculate `y`\ny &lt;- rpois(length(x), lambda)\n\n# combine `x` and `y`\ndat &lt;-  data.frame(x = x, y = y)\nHere’s the first few rows of our simulated data\nx\ny\n\n\n\n\n0.2875775\n5\n\n\n0.7883051\n11\n\n\n0.4089769\n6\n\n\n0.8830174\n20\n\n\n0.9404673\n18\n\n\n0.0455565\n1\nAnd here is a simple scatterplot of the data showing a clear positive relationship between x and y\nlibrary(ggplot2)\nlibrary(cowplot)\n\nggplot(dat, aes(x = x, y = y)) + \n    geom_point() +\n    theme_cowplot()\nNow we can build a model of y as it responds to x:\nmod &lt;- glm(y ~ x, family = poisson)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Likelihood, AIC, and likelihood ratio tests</span>"
    ]
  },
  {
    "objectID": "likelihood.html#what-is-a-model",
    "href": "likelihood.html#what-is-a-model",
    "title": "1  Likelihood, AIC, and likelihood ratio tests",
    "section": "1.2 What is a model?",
    "text": "1.2 What is a model?\nLet’s have a look at what R tells us about this model we just made:\n\nmod\n\n\nCall:  glm(formula = y ~ x, family = poisson)\n\nCoefficients:\n(Intercept)            x  \n     0.8888       2.1178  \n\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\nNull Deviance:      353.5 \nResidual Deviance: 79.36    AIC: 457.7\n\n\nWe see there are estimated coefficients: (Intercept) and x. The intercept is just that, the intercept, and the x coefficient is actually the slope. But what intercept and what slope? These words imply an equation, and in fact the equation was hiding in the code we used to simulate data. We said\n\n# calculate the mean of `y` as a function of `x`\nlambda &lt;- exp(1 + 2 * x)\n\n# finally calculate `y`\ny &lt;- rpois(length(x), lambda)\n\nThe equation hiding in this code is\n\\[\\begin{align}\ny &\\sim Pois(\\lambda) \\text{, where} \\\\\n\\lambda &= \\exp(b_0 + b_1 x)\n\\end{align}\\]\nThis says that \\(y\\) is distributed according to a Poisson distribution (because we’re doing Poisson GLM) and that the mean of this Poisson distribution is a function of \\(x\\). Specifically, that function of \\(x\\) is \\(\\lambda = \\exp(b_0 + b_1 x)\\). So the slope is \\(b_1\\) and the intercept is \\(b_0\\). If we log transformed both sides we can more clearly see how \\(b_0\\) is the intercept and \\(b_1\\) the slope:\n\\[\\begin{align}\n&\\log(\\lambda) = \\log(\\exp(b_0 + b_1 x)) \\\\\n\\Rightarrow &\\log(\\lambda) = b_0 + b_1 x\n\\end{align}\\]\nThat we take the log of \\(\\lambda\\) is consistent with the fact that the Poisson GLM uses a log “link function” by default.\nIn our simulated data we set \\(b_0 = 1\\) and \\(b_1 = 2\\). Looking back at the output of the glm function we can see that our estimates are very close to those values!\n\nmod\n\n\nCall:  glm(formula = y ~ x, family = poisson)\n\nCoefficients:\n(Intercept)            x  \n     0.8888       2.1178  \n\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\nNull Deviance:      353.5 \nResidual Deviance: 79.36    AIC: 457.7\n\n\nWe can directly access those estimates like this:\n\nb0_est &lt;- mod$coefficients[1]\nb1_est &lt;- mod$coefficients[2]\n\nb0_est\n\n(Intercept) \n  0.8887906 \n\nb1_est\n\n       x \n2.117753 \n\n\nBut how are those estimates actually made? That’s the job for likelihood.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Likelihood, AIC, and likelihood ratio tests</span>"
    ]
  },
  {
    "objectID": "likelihood.html#what-is-a-likelihood",
    "href": "likelihood.html#what-is-a-likelihood",
    "title": "1  Likelihood, AIC, and likelihood ratio tests",
    "section": "1.3 What is a likelihood?",
    "text": "1.3 What is a likelihood?\ntl;dr: likelihood is the probability of the data given the model and its parameters. The parameter estimates for our model are exactly the parameter values that produce the maximum possible likelihood of the data. We call them the maximum likelihood estimates.\nWhat does that mean? Let’s start with “probability of the data given the model and its parameters”. Remember from our equations, we are modeling \\(y\\) as coming from a Poisson distribution with mean \\(\\lambda\\), \\(\\lambda\\) is itself a function of \\(x\\) with parameters \\(b_0\\) and \\(b_1\\). So if we want to know the probability of any given data point (we’ll call any given data point \\(y_i\\)) then we just need to ask for the probability from the Poisson distribution, like this:\n\ni &lt;- 6 # let's look at the 6th data point\ny_i &lt;- dat$y[i]\n\n# calculate lambda for data point i\nlambda_i &lt;- exp(b0_est + b1_est * dat$x[i])\n\ndpois(y_i, lambda = lambda_i)\n\n[1] 0.1839187\n\n\nThat’s the probability of one data point, what about the probability of the entire dataset? Recall that probabilities multiply for independent observations, so the probability of the entire dataset is\n\\[\\begin{align}\nP(Y) &= P(y_1) \\times P(y_2) \\times P(y_2) \\times \\cdots \\times P(y_n) \\\\\n\\Rightarrow P(Y) &= \\prod_{i = 1}^n P(y_i)\n\\end{align}\\]\nThis probability \\(P(Y)\\) is exactly the likelihood. We might write it as\n\\[\n\\mathcal{L}(Y | b_0, b_1) = \\prod_{i = 1}^n P(y_i | b_0, b_1)\n\\]\nWe added the “\\(| b_0, b_1\\)” to emphasize that the probability and likelihood depend on the values of \\(b_0\\) and \\(b_1\\).\nIn R code we can calculate that math like this:\n\nallProbs &lt;- dpois(y, exp(b0_est + b1_est * x))\nprod(allProbs)\n\n[1] 3.08673e-99\n\n\nShoot! The probability is \\(3.0867301\\times 10^{-99}\\), so….basically 0. This is why when working with likelihoods we actually use the log likelihood. Taking the log of a product (i.e. multiplication) turns it into summation. We typically use a little \\(\\mathcal{l}\\) for log likelihood. So our math becomes\n\\[\\begin{align}\n&\\log(\\mathcal{L}(Y | b_0, b_1)) = \\log\\left( \\prod_{i = 1}^n P(y_i | b_0, b_1) \\right) \\\\\n\\Rightarrow &\\mathcal{l}(Y | b_0, b_1) = \\sum_{i = 1}^n \\log\\left( P(y_i | b_0, b_1) \\right)\n\\end{align}\\]\nAnd our code becomes\n\nallLogProbs &lt;- dpois(y, exp(b0_est + b1_est * x), log = TRUE)\nsum(allLogProbs)\n\n[1] -226.8288\n\n\nSweet! Our log likelihood is negative (no problem) and a reasonable number, not something basically equal to 0.\nHow does this help us find our parameter estimates? To find out, let’s calculate the log likelihood of our data, but with a different value for the slope, say \\(b_1 = -1\\).\n\nl_neg1 &lt;- dpois(y, exp(b0_est - 1 * x), log = TRUE)\nsum(l_neg1)\n\n[1] -1259.075\n\n\nThat’s a much more negative log likelihood!\nLet’s try another value, say \\(b_1 = 4\\)\n\nl_pos4 &lt;- dpois(y, exp(b0_est + 4 * x), log = TRUE)\nsum(l_pos4)\n\n[1] -1556.177\n\n\nThat is also a much more negative number compared to the log likelihood at the actual estimated slope that R gave us. If we made a graph of how the log likelihood changes across different possible values of \\(b_1\\) it would look like this:\n\n\n\n\n\n\n\n\n\nThe log likelihood reaches its maximum value at exactly the value of \\(b_1\\) which the glm function gives us (shown in red).\nSo the parameter estimates that maximize the log likelihood of the data are the best possible parameter estimates for our model! And that’s how likelihood allows us to estimate parameters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Likelihood, AIC, and likelihood ratio tests</span>"
    ]
  },
  {
    "objectID": "likelihood.html#akaike-information-criterion",
    "href": "likelihood.html#akaike-information-criterion",
    "title": "1  Likelihood, AIC, and likelihood ratio tests",
    "section": "1.4 Akaike Information Criterion",
    "text": "1.4 Akaike Information Criterion\nSo we can use log likelihood to estimate parameters, anything else? Yes! For one thing we can use log likelihood to help us decide between competing models. Competing models is the scenario where you have more than one possible model that you think could predict the data at hand and you want to decide which one does the best job relative to all the others.\nIn our simple example where we have y as some kind of count data and x as an explanatory variable, competing models could mean, for example, we have another explanatory variable w and we want to know which variable(s) do(es) the best job at predicting y.\nLet’s make this concrete with code and some more simulated data\n\n# simulate w as random numbers that in fact have nothing to do with y\nw &lt;- runif(length(x))\n\n# add w to our data.frame\ndat$w &lt;- w\n\n# visualize y versus x and y versus w\npx &lt;- ggplot(dat, aes(x = x, y = y)) + \n    geom_point() +\n    theme_cowplot()\n\npw &lt;- ggplot(dat, aes(x = w, y = y)) + \n    geom_point() +\n    theme_cowplot()\n\nplot_grid(px, pw, nrow = 1)\n\n\n\n\n\n\n\n\nAs we intended there is no real trend in y across w. But let’s compare the likelihoods of the model with only x and a model with x and w:\n\nmod_with_w &lt;- glm(y ~ x + w, family = poisson)\n\n\nlogLik(mod_with_w)\n\n'log Lik.' -226.8016 (df=3)\n\nlogLik(mod)\n\n'log Lik.' -226.8288 (df=2)\n\n\nThe likelihood of the model with w is actually slightly higher (i.e. less negative) than the model with only x. He aha lā?! It turns out that adding more parameters to a model (even if they’re attached to nonsense explanatory variables) will always improve the likelihood of the data given the model. This is because every added parameter allows the model to capture a little more of the noise in the data, thus increasing the probability of the data given the model—that is, improving the likelihood. But we actually don’t want to fit our model to the noise, we want to fit our model to the real biology of what’s going on in the data.\nSo how can we decide which model is best when simply adding nonsense paramters will always improve the likelihood? This is the job of Akaike Information Criterion or “AIC”. AIC is a metric that describes how much a model is supported by the data. It is directly related to the log likelihood, but it penalizes models for the number of parameters they include, the more parameters, the higher the penalization. The equation for AIC is\n\\[\nAIC = 2k - 2l\n\\] where \\(k\\) is the number of fitted parameters in the model and \\(l\\) is the log likelihood. Let’s look at the AIC values for our two models\n\nAIC(mod_with_w)\n\n[1] 459.6033\n\nAIC(mod)\n\n[1] 457.6576\n\n\nA smaller AIC indicates better model support, and sure enough the AIC of the model with only x is smaller by about 2 points.\nAIC is helpful for telling us which model is relatively more supported by the data, but it does not tell us at all if the best supported model is actually any good. Just that it’s better than the other options. To figure out if the model is actually any good at predicting the data, we need a different approach.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Likelihood, AIC, and likelihood ratio tests</span>"
    ]
  },
  {
    "objectID": "likelihood.html#likelihood-ratio-test",
    "href": "likelihood.html#likelihood-ratio-test",
    "title": "1  Likelihood, AIC, and likelihood ratio tests",
    "section": "1.5 Likelihood ratio test",
    "text": "1.5 Likelihood ratio test\nEnter the likelihood ratio test—this can actually give us some insight as to whether our model is any good at predicting the data. We often abbreviate the likelihood ratio test as LRT, we’ll adopt that starting now.\nIn the case of our preferred model of y as a function of x, to know if this model is any good, we need to figure out if knowing x actually helps us predict the values of y. Put another way, is modeling y as a function of x any more predictive than a model with only an intercept (i.e. a flat line through the mean of y)? The LRT answers that question for us through a null hypothesis test that produces a \\(p\\)-value. The null hypothesis is that the likelihood for the flat line model with only intercept is statistically indistinguishable from the likelihood for the full model, the over with intercept and a slope for x.\nWe often call the model with fewer parameters, in this case the over with just the intercept, the “reduced model”. Let’s indicate the full model with all its parameters as \\(m\\) and the reduced model as \\(m_0\\). Then our two log likelihoods are\n\\[\\begin{align}\n\\mathcal{l}_0 &= \\log \\mathcal{L}(y | m_0) \\\\\n\\mathcal{l} &= \\log \\mathcal{L}(y | m)\n\\end{align}\\]\nAnd the test statistic for the LRT is\n\\[\nLRT = -2 (\\mathcal{l}_0 - \\mathcal{l})\n\\]\nYou might be wondering, where is the ratio (aka fraction) in this test statistic? Recall that logarithms have some special rules so that\n\\[\n-2 (\\mathcal{l}_0 - \\mathcal{l}) = \\log\\left( \\frac{\\mathcal{L}(y | m_0)}{\\mathcal{L}(y | m)} \\right)\n\\]\nSo the LRT test statistic is \\(-2\\) times the log of the ratio of the reduced model likelihood over the full model likelihood.\nIt turns out that under the null hypothesis—of indistinguishable likelihoods—this test statistic has a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of fitted parameters between the models. The reduced model has one fitted parameter, the intercept, and the full model has two, intercept and slope. So the degrees of freedom for the null \\(\\chi^2\\) distribution.\nLet’s calculate the \\(LRT\\) test statistic for our analysis and then calculate the \\(P\\)-value from the test statistic and relevant \\(\\chi^2\\) distribution\n\n# first we need to make the reduced model\n# the formula `y ~ 1` is how we tell R to just estimate the intercept, no slopes\nmod_0 &lt;- glm(y ~ 1, data = dat, family = poisson)\n\n# now we can calculate lrt = -2 * (l_0 - l)\nlrt &lt;- -2 * (logLik(mod_0) - logLik(mod))\n\n# finally let's calculate the p-value by seeing where this test statistic \n# falls with repsect to the upper tail of the chi^2 distribution\npchisq(lrt, df = 1, lower.tail = FALSE)\n\n'log Lik.' 1.409661e-61 (df=1)\n\n\nSo the \\(P\\)-value is \\(1.4096614\\times 10^{-61}\\), which is basically 0, and indeed that is less than our typical \\(\\alpha = 0.05\\) cutoff for statistical significance. So we reject the null hypothesis of indistinguishable likelihoods and conclude that the model including a slope for x indeed does a better job predicting the data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Likelihood, AIC, and likelihood ratio tests</span>"
    ]
  },
  {
    "objectID": "likelihood.html#key-differences-between-aic-and-lrt",
    "href": "likelihood.html#key-differences-between-aic-and-lrt",
    "title": "1  Likelihood, AIC, and likelihood ratio tests",
    "section": "1.6 Key differences between AIC and LRT",
    "text": "1.6 Key differences between AIC and LRT\nOne key difference we already know is that AIC does not tell us if the model is a good fit to the data, it only helps us choose the best model out of two or more models that we are considering. All the models could in fact be bad and then AIC will just tell us which is the least bad. Conversely, the LRT can actually tell us if the model we’re interested in does a better job of predicting the data than a more simplified model. If our model of interest is better at prediction than a more simplified model, we have reason to conclude our model is actually relevant and meaningful.\nAnother key difference between AIC and LRT is when each approach is statistically appropriate and not appropriate. AIC can technically be used to compare any collection of models, while the LRT only works for “nested models.” What are nested models? They are any two models where the “simpler” model contains a subset of the parameters from the full model and no other parameters not contained in the full model. So the example we’ve been working with of \\(m_0\\) and \\(m\\) meets this criterion. We can see this clearly if we write the equations for each.\nThe full model is:\n\\[\\begin{align}\nY &\\sim Pois(\\lambda) \\\\\n\\lambda &= exp(b_0 + b_1 x)\n\\end{align}\\]\nThe reduced model is:\n\\[\\begin{align}\nY &\\sim Pois(\\lambda_0) \\\\\n\\lambda_0 &= exp(b_0)\n\\end{align}\\]\nSo the reduced model and full model both have parameter \\(b_0\\), but only the full model has parameter \\(b_1\\), this means the parameters of the reduced model are a subset of, are “nested within,” the parameters of the full model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Likelihood, AIC, and likelihood ratio tests</span>"
    ]
  }
]